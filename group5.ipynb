{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f207db",
   "metadata": {},
   "source": [
    "# Energy Anomaly & Automated Power Theft Detection System  \n",
    "### A Data Science Research Framework for Context-Aware Grid Intelligence\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Electricity utilities in **Kenya** face significant financial and operational strain due to non-technical losses arising from electricity theft, meter tampering, illegal connections, and irregular consumption behavior. While smart meter infrastructure generates large volumes of high-frequency time-series data, many utilities lack intelligent systems capable of converting raw consumption signals into actionable risk alerts.\n",
    "\n",
    "This project develops an end-to-end data science framework that not only detects abnormal electricity usage patterns but also generates structured, automated risk notifications suitable for investigation workflows.\n",
    "\n",
    "To simulate a realistic operational environment, a multi-household electricity dataset is constructed using high-resolution consumption measurements. Natural behavioral variability is preserved across households, while selected households are injected with synthetic theft-like patterns such as sustained consumption drops and altered load distributions. This enables controlled validation of anomaly detection techniques in the absence of real labeled fraud data.\n",
    "\n",
    "The system integrates three core data layers:\n",
    "\n",
    "1. **Electricity Consumption Data (Behavioral Signal Layer)**  \n",
    "   Minute-level power and voltage readings aggregated into structured daily behavioral indicators.\n",
    "\n",
    "2. **Weather Data (Environmental Context Layer)**  \n",
    "   Temperature, precipitation, and wind speed variables used to explain legitimate demand variability and reduce false anomaly detection.\n",
    "\n",
    "3. **Scheduled Outage Information (Operational Filter Layer)**  \n",
    "   Official maintenance interruption records structured into daily indicators to prevent misclassification of planned supply disruptions.\n",
    "\n",
    "The analytical pipeline transitions from raw time-series inputs to a structured intelligence system that:\n",
    "\n",
    "- Engineers behavioral and change-based features  \n",
    "- Adjusts signals using environmental and operational context  \n",
    "- Applies unsupervised anomaly detection techniques  \n",
    "- Assigns quantitative theft-risk scores  \n",
    "- Triggers automated structured notification outputs for high-risk cases  \n",
    "\n",
    "The final system moves beyond static classification by producing prioritized, investigation-ready alerts supported by explainable risk indicators. This framework demonstrates how utilities can transition from reactive inspection-based fraud handling to proactive, data-driven anomaly intelligence with automated alert generation.\n",
    "\n",
    "---\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "Electricity utilities operate in environments where revenue protection, grid reliability, and operational efficiency are critical. A major persistent challenge is the presence of non-technical losses caused by electricity theft and irregular consumption behavior.\n",
    "\n",
    "These losses:\n",
    "\n",
    "- Reduce utility revenue  \n",
    "- Increase operational and inspection costs  \n",
    "- Introduce uneven demand stress on distribution infrastructure  \n",
    "- Compromise grid stability  \n",
    "\n",
    "Traditional fraud detection approaches rely on:\n",
    "\n",
    "- Manual inspections  \n",
    "- Customer complaints  \n",
    "- Rule-based heuristics  \n",
    "\n",
    "These methods are reactive, costly, and inefficient.\n",
    "\n",
    "Although smart meters provide high-frequency consumption data, most utilities lack structured systems capable of distinguishing legitimate variability (e.g., weather shifts, seasonal effects, scheduled outages) from suspicious behavioral anomalies. Furthermore, even when anomalies are detected, many utilities lack automated mechanisms to translate analytical outputs into actionable investigation alerts.\n",
    "\n",
    "The central business problem addressed in this project is:\n",
    "\n",
    "> How can utilities leverage integrated consumption, environmental, and operational data to proactively detect abnormal electricity behavior and automatically generate structured investigation notifications?\n",
    "\n",
    "Specifically, the challenge involves:\n",
    "\n",
    "- Detecting anomalous patterns without fully labeled theft data  \n",
    "- Minimizing false positives caused by legitimate variability  \n",
    "- Translating anomaly scores into explainable risk indicators  \n",
    "- Automatically producing structured alerts to support investigation workflows  \n",
    "- Designing a scalable, context-aware detection framework suitable for operational deployment  \n",
    "\n",
    "This project addresses these challenges by developing a layered anomaly detection system with an embedded automated notification mechanism that flags high-risk consumption cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Objectives\n",
    "\n",
    "The primary objective of this project is to design, implement, and evaluate a context-aware anomaly detection framework capable of identifying potential power theft and generating automated risk notifications using time-series smart meter data.\n",
    "\n",
    "### 1 Data Preparation & Simulation\n",
    "\n",
    "- Construct a multi-household electricity consumption dataset from high-frequency readings.\n",
    "- Introduce controlled behavioral diversity across simulated households.\n",
    "- Inject theft-like consumption patterns to enable controlled anomaly validation.\n",
    "\n",
    "### 2 Feature Engineering\n",
    "\n",
    "- Aggregate minute-level consumption into structured daily indicators.\n",
    "- Engineer statistical and volatility-based features.\n",
    "- Create change-based indicators (rolling averages, percentage shifts).\n",
    "- Integrate weather variables for contextual adjustment.\n",
    "- Incorporate scheduled outage indicators as operational filters.\n",
    "\n",
    "### 3 Anomaly Detection Modeling\n",
    "\n",
    "- Apply unsupervised anomaly detection techniques (e.g., Isolation Forest).\n",
    "- Generate quantitative anomaly scores per household-day.\n",
    "- Define risk thresholds to classify consumption into Low, Medium, and High-risk categories.\n",
    "\n",
    "### 4 Evaluation & Validation\n",
    "\n",
    "- Measure detection consistency across simulated theft scenarios.\n",
    "- Analyze false positives resulting from weather or outage effects.\n",
    "- Assess stability of anomaly detection across heterogeneous households.\n",
    "\n",
    "### 5 Automated Risk Notification Layer\n",
    "\n",
    "- Develop a structured alert-generation mechanism triggered by defined anomaly thresholds.\n",
    "- Create investigation-ready outputs including:\n",
    "  - Meter ID\n",
    "  - Date\n",
    "  - Risk score\n",
    "  - Risk category\n",
    "  - Supporting behavioral indicators\n",
    "- Demonstrate how anomaly detection outputs can feed into downstream notification workflows (e.g., case export, dashboard alerting, automated email triggers).\n",
    "Through these objectives, the project demonstrates how integrated data science techniques can power a proactive energy irregularity detection system that combines anomaly modeling with automated alert generation.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e965dbd",
   "metadata": {},
   "source": [
    "## Research Phase I — Anomaly Detection Engine Development (LEAD Dataset)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Before deploying the electricity theft detection system on the Kenya dataset, we first develop and validate a robust anomaly detection engine using the LEAD (Large-scale Energy Anomaly Detection) dataset.\n",
    "\n",
    "The LEAD dataset contains hourly smart meter readings from multiple buildings, along with labeled anomaly events. This makes it ideal for calibrating anomaly detection algorithms and evaluating performance before production deployment.\n",
    "\n",
    "---\n",
    "\n",
    "### Why We Start With LEAD\n",
    "\n",
    "The Kenya deployment dataset does not contain confirmed theft labels. Therefore, building a model directly on it would make it difficult to evaluate performance objectively.\n",
    "\n",
    "Using LEAD allows us to:\n",
    "\n",
    "- Validate anomaly detection techniques on labeled data\n",
    "- Evaluate precision and recall of detected anomalies\n",
    "- Tune model parameters appropriately\n",
    "- Avoid overfitting or under-sensitive detection in production\n",
    "\n",
    "---\n",
    "\n",
    "### What We Will Do in This Phase\n",
    "\n",
    "1. Load and inspect the dataset structure  \n",
    "2. Assess anomaly class imbalance  \n",
    "3. Handle missing values  \n",
    "4. Engineer time-based features  \n",
    "5. Build an anomaly detection model (Isolation Forest)  \n",
    "6. Evaluate model performance using anomaly labels  \n",
    "\n",
    "---\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "At the end of this phase, we will have:\n",
    "\n",
    "- A validated anomaly detection engine  \n",
    "- Performance metrics (precision, recall, F1-score)  \n",
    "- A calibrated detection threshold  \n",
    "- A robust foundation for deployment in the Kenya electricity theft detection system  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a458c522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id            timestamp  meter_reading  anomaly\n",
       "0            1  2016-01-01 00:00:00            NaN        0\n",
       "1           32  2016-01-01 00:00:00            NaN        0\n",
       "2           41  2016-01-01 00:00:00            NaN        0\n",
       "3           55  2016-01-01 00:00:00            NaN        0\n",
       "4           69  2016-01-01 00:00:00            NaN        0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lead_df = pd.read_csv(\"lead1.0-small.csv\")\n",
    "\n",
    "lead_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8e5c910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1749494 entries, 0 to 1749493\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   building_id    int64  \n",
      " 1   timestamp      object \n",
      " 2   meter_reading  float64\n",
      " 3   anomaly        int64  \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 53.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lead_df.info()\n",
    "lead_df.isnull().sum()\n",
    "lead_df[\"anomaly\"].value_counts()\n",
    "lead_df[\"building_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5e7077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing % in meter_reading: 6.15 %\n",
      "\n",
      "Anomaly distribution:\n",
      "anomaly\n",
      "0    97.868184\n",
      "1     2.131816\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  timestamp  meter_reading  anomaly\n",
       "0            1 2016-01-01            NaN        0\n",
       "1           32 2016-01-01            NaN        0\n",
       "2           41 2016-01-01            NaN        0\n",
       "3           55 2016-01-01            NaN        0\n",
       "4           69 2016-01-01            NaN        0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert timestamp to datetime\n",
    "lead_df[\"timestamp\"] = pd.to_datetime(lead_df[\"timestamp\"])\n",
    "\n",
    "# Check missing percentage\n",
    "missing_percent = lead_df[\"meter_reading\"].isna().mean() * 100\n",
    "print(\"Missing % in meter_reading:\", round(missing_percent, 2), \"%\")\n",
    "\n",
    "# Anomaly distribution\n",
    "print(\"\\nAnomaly distribution:\")\n",
    "print(lead_df[\"anomaly\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "lead_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a99ca3",
   "metadata": {},
   "source": [
    "## Data Cleaning and Time-Series Preparation\n",
    "\n",
    "The LEAD dataset contains approximately 6.15% missing values in the `meter_reading` column. Since this dataset represents hourly time-series data, dropping rows could disrupt temporal continuity.\n",
    "\n",
    "Therefore, we perform:\n",
    "\n",
    "- Sorting by `building_id` and `timestamp`\n",
    "- Group-based interpolation to preserve temporal structure\n",
    "- Validation to ensure missing values are resolved appropriately\n",
    "\n",
    "Maintaining time-series integrity is critical for anomaly detection performance, especially when using models such as Isolation Forest that rely on consistent feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f863ce76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing: 28026\n"
     ]
    }
   ],
   "source": [
    "# Sort properly\n",
    "lead_df = lead_df.sort_values([\"building_id\", \"timestamp\"])\n",
    "\n",
    "# Interpolate missing meter readings per building\n",
    "lead_df[\"meter_reading\"] = (\n",
    "    lead_df.groupby(\"building_id\")[\"meter_reading\"]\n",
    "    .transform(lambda x: x.interpolate(method=\"linear\"))\n",
    ")\n",
    "\n",
    "# Check remaining missing values\n",
    "print(\"Remaining missing:\", lead_df[\"meter_reading\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e7559a",
   "metadata": {},
   "source": [
    "### Handling Edge Missing Values\n",
    "\n",
    "After interpolation, some missing values remain at the beginning or end of building time series. These occur because interpolation requires surrounding values.\n",
    "\n",
    "To preserve time continuity, we apply forward-fill and backward-fill within each building group. This ensures complete time-series integrity before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ff41071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing after fill: 0\n"
     ]
    }
   ],
   "source": [
    "# Forward fill per building\n",
    "lead_df[\"meter_reading\"] = (\n",
    "    lead_df.groupby(\"building_id\")[\"meter_reading\"]\n",
    "    .transform(lambda x: x.ffill().bfill())\n",
    ")\n",
    "\n",
    "# Final check\n",
    "print(\"Remaining missing after fill:\", lead_df[\"meter_reading\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea1b86",
   "metadata": {},
   "source": [
    "## Time Feature Engineering\n",
    "\n",
    "Electricity consumption is highly dependent on time-based patterns such as hour of day, day of week, and seasonal effects.\n",
    "\n",
    "Before building our anomaly detection model, we extract temporal features from the timestamp column. These features will allow the model to understand:\n",
    "\n",
    "- Hourly consumption cycles  \n",
    "- Weekly behavioral patterns  \n",
    "- Seasonal/monthly effects  \n",
    "\n",
    "Feature engineering is critical because anomaly detection models rely on statistical structure. Without time-based context, normal cyclical patterns may be incorrectly flagged as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0546358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     building_id           timestamp  meter_reading  anomaly  hour  \\\n",
       "0              1 2016-01-01 00:00:00         27.688        0     0   \n",
       "200            1 2016-01-01 01:00:00         27.688        0     1   \n",
       "398            1 2016-01-01 02:00:00         27.688        0     2   \n",
       "597            1 2016-01-01 03:00:00         27.688        0     3   \n",
       "796            1 2016-01-01 04:00:00         27.688        0     4   \n",
       "\n",
       "     day_of_week  month  day  \n",
       "0              4      1    1  \n",
       "200            4      1    1  \n",
       "398            4      1    1  \n",
       "597            4      1    1  \n",
       "796            4      1    1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract time-based features\n",
    "lead_df[\"hour\"] = lead_df[\"timestamp\"].dt.hour\n",
    "lead_df[\"day_of_week\"] = lead_df[\"timestamp\"].dt.dayofweek\n",
    "lead_df[\"month\"] = lead_df[\"timestamp\"].dt.month\n",
    "lead_df[\"day\"] = lead_df[\"timestamp\"].dt.day\n",
    "\n",
    "lead_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d62389",
   "metadata": {},
   "source": [
    "## Rolling Baseline and Residual Computation\n",
    "\n",
    "Electricity consumption follows repeating daily and weekly patterns. To detect anomalies effectively, we must first estimate what “normal” consumption looks like.\n",
    "\n",
    "Instead of relying on raw meter readings, we compute a rolling baseline per building. This allows us to compare actual consumption against expected behavior.\n",
    "\n",
    "Steps performed:\n",
    "\n",
    "- Calculate rolling mean (expected consumption)\n",
    "- Calculate rolling standard deviation\n",
    "- Compute residual (actual − expected)\n",
    "- Compute standardized deviation (z-score)\n",
    "\n",
    "These features form the backbone of our anomaly detection engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74e69444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>rolling_mean</th>\n",
       "      <th>rolling_std</th>\n",
       "      <th>residual</th>\n",
       "      <th>z_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 05:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 06:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 07:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 08:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 09:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 10:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 11:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 12:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 13:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 14:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 15:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 16:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 17:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3582</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 18:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 19:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 20:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 21:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 22:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 23:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02 00:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02 01:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5173</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02 02:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5372</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02 03:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02 04:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5770</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02 05:00:00</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      building_id           timestamp  meter_reading  anomaly  hour  \\\n",
       "0               1 2016-01-01 00:00:00         27.688        0     0   \n",
       "200             1 2016-01-01 01:00:00         27.688        0     1   \n",
       "398             1 2016-01-01 02:00:00         27.688        0     2   \n",
       "597             1 2016-01-01 03:00:00         27.688        0     3   \n",
       "796             1 2016-01-01 04:00:00         27.688        0     4   \n",
       "995             1 2016-01-01 05:00:00         27.688        0     5   \n",
       "1194            1 2016-01-01 06:00:00         27.688        0     6   \n",
       "1393            1 2016-01-01 07:00:00         27.688        0     7   \n",
       "1592            1 2016-01-01 08:00:00         27.688        0     8   \n",
       "1791            1 2016-01-01 09:00:00         27.688        0     9   \n",
       "1990            1 2016-01-01 10:00:00         27.688        0    10   \n",
       "2189            1 2016-01-01 11:00:00         27.688        0    11   \n",
       "2388            1 2016-01-01 12:00:00         27.688        0    12   \n",
       "2587            1 2016-01-01 13:00:00         27.688        0    13   \n",
       "2786            1 2016-01-01 14:00:00         27.688        0    14   \n",
       "2985            1 2016-01-01 15:00:00         27.688        0    15   \n",
       "3184            1 2016-01-01 16:00:00         27.688        0    16   \n",
       "3383            1 2016-01-01 17:00:00         27.688        0    17   \n",
       "3582            1 2016-01-01 18:00:00         27.688        0    18   \n",
       "3781            1 2016-01-01 19:00:00         27.688        0    19   \n",
       "3980            1 2016-01-01 20:00:00         27.688        0    20   \n",
       "4179            1 2016-01-01 21:00:00         27.688        0    21   \n",
       "4378            1 2016-01-01 22:00:00         27.688        0    22   \n",
       "4577            1 2016-01-01 23:00:00         27.688        0    23   \n",
       "4776            1 2016-01-02 00:00:00         27.688        0     0   \n",
       "4975            1 2016-01-02 01:00:00         27.688        0     1   \n",
       "5173            1 2016-01-02 02:00:00         27.688        0     2   \n",
       "5372            1 2016-01-02 03:00:00         27.688        0     3   \n",
       "5571            1 2016-01-02 04:00:00         27.688        0     4   \n",
       "5770            1 2016-01-02 05:00:00         27.688        0     5   \n",
       "\n",
       "      day_of_week  month  day  rolling_mean  rolling_std  residual  z_score  \n",
       "0               4      1    1        27.688          NaN       0.0      NaN  \n",
       "200             4      1    1        27.688          0.0       0.0      0.0  \n",
       "398             4      1    1        27.688          0.0       0.0      0.0  \n",
       "597             4      1    1        27.688          0.0       0.0      0.0  \n",
       "796             4      1    1        27.688          0.0       0.0      0.0  \n",
       "995             4      1    1        27.688          0.0       0.0      0.0  \n",
       "1194            4      1    1        27.688          0.0       0.0      0.0  \n",
       "1393            4      1    1        27.688          0.0       0.0      0.0  \n",
       "1592            4      1    1        27.688          0.0       0.0      0.0  \n",
       "1791            4      1    1        27.688          0.0       0.0      0.0  \n",
       "1990            4      1    1        27.688          0.0       0.0      0.0  \n",
       "2189            4      1    1        27.688          0.0       0.0      0.0  \n",
       "2388            4      1    1        27.688          0.0       0.0      0.0  \n",
       "2587            4      1    1        27.688          0.0       0.0      0.0  \n",
       "2786            4      1    1        27.688          0.0       0.0      0.0  \n",
       "2985            4      1    1        27.688          0.0       0.0      0.0  \n",
       "3184            4      1    1        27.688          0.0       0.0      0.0  \n",
       "3383            4      1    1        27.688          0.0       0.0      0.0  \n",
       "3582            4      1    1        27.688          0.0       0.0      0.0  \n",
       "3781            4      1    1        27.688          0.0       0.0      0.0  \n",
       "3980            4      1    1        27.688          0.0       0.0      0.0  \n",
       "4179            4      1    1        27.688          0.0       0.0      0.0  \n",
       "4378            4      1    1        27.688          0.0       0.0      0.0  \n",
       "4577            4      1    1        27.688          0.0       0.0      0.0  \n",
       "4776            5      1    2        27.688          0.0       0.0      0.0  \n",
       "4975            5      1    2        27.688          0.0       0.0      0.0  \n",
       "5173            5      1    2        27.688          0.0       0.0      0.0  \n",
       "5372            5      1    2        27.688          0.0       0.0      0.0  \n",
       "5571            5      1    2        27.688          0.0       0.0      0.0  \n",
       "5770            5      1    2        27.688          0.0       0.0      0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 24-hour rolling window baseline per building\n",
    "window_size = 24\n",
    "\n",
    "lead_df[\"rolling_mean\"] = (\n",
    "    lead_df.groupby(\"building_id\")[\"meter_reading\"]\n",
    "    .transform(lambda x: x.rolling(window_size, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "lead_df[\"rolling_std\"] = (\n",
    "    lead_df.groupby(\"building_id\")[\"meter_reading\"]\n",
    "    .transform(lambda x: x.rolling(window_size, min_periods=1).std())\n",
    ")\n",
    "\n",
    "# Residual and standardized deviation\n",
    "lead_df[\"residual\"] = lead_df[\"meter_reading\"] - lead_df[\"rolling_mean\"]\n",
    "\n",
    "lead_df[\"z_score\"] = lead_df[\"residual\"] / (lead_df[\"rolling_std\"] + 1e-6)\n",
    "\n",
    "lead_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ebcb74",
   "metadata": {},
   "source": [
    "## Isolation Forest — Unsupervised Anomaly Detection\n",
    "\n",
    "With rolling baseline features engineered, we now build our first anomaly detection model using Isolation Forest.\n",
    "\n",
    "Isolation Forest is particularly suited for anomaly detection because:\n",
    "\n",
    "- It isolates rare observations efficiently\n",
    "- It performs well on high-dimensional data\n",
    "- It does not require labeled data\n",
    "- It handles imbalance naturally\n",
    "\n",
    "In this step, we train the model using engineered time-series features and evaluate its ability to detect labeled anomalies in the LEAD dataset.\n",
    "\n",
    "The goal is to calibrate our anomaly detection engine before production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb8045e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1679305   32893]\n",
      " [  35199    2097]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98   1712198\n",
      "           1       0.06      0.06      0.06     37296\n",
      "\n",
      "    accuracy                           0.96   1749494\n",
      "   macro avg       0.52      0.52      0.52   1749494\n",
      "weighted avg       0.96      0.96      0.96   1749494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature selection\n",
    "features = [\n",
    "    \"meter_reading\",\n",
    "    \"residual\",\n",
    "    \"z_score\",\n",
    "    \"hour\",\n",
    "    \"day_of_week\",\n",
    "    \"month\"\n",
    "]\n",
    "\n",
    "X = lead_df[features]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.02,  # ~2% anomaly rate\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lead_df[\"iso_pred\"] = iso.fit_predict(X_scaled)\n",
    "\n",
    "# Convert predictions: -1 = anomaly → 1, 1 = normal → 0\n",
    "lead_df[\"iso_pred\"] = lead_df[\"iso_pred\"].map({1: 0, -1: 1})\n",
    "\n",
    "# Evaluation\n",
    "print(confusion_matrix(lead_df[\"anomaly\"], lead_df[\"iso_pred\"]))\n",
    "print(classification_report(lead_df[\"anomaly\"], lead_df[\"iso_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7ff2a",
   "metadata": {},
   "source": [
    "## Initial Isolation Forest Results\n",
    "\n",
    "The first Isolation Forest model achieved high overall accuracy (96%). However, due to severe class imbalance (only ~2% anomalies), accuracy is not a reliable metric for evaluating anomaly detection performance.\n",
    "\n",
    "For the anomaly class:\n",
    "\n",
    "- Precision: 0.06  \n",
    "- Recall: 0.06  \n",
    "- F1-score: 0.06  \n",
    "\n",
    "This indicates that the model is detecting only a small fraction of true anomalies while generating a considerable number of false positives.\n",
    "\n",
    "The weak anomaly performance suggests that the current feature space does not sufficiently capture temporal consumption behavior. In particular, electricity usage patterns are seasonal and cyclical, and a simple 24-hour rolling baseline may not adequately represent weekly consumption dynamics.\n",
    "\n",
    "To improve anomaly detection quality, we will refine the baseline modeling strategy to incorporate longer seasonal windows (e.g., weekly patterns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920fad7",
   "metadata": {},
   "source": [
    "## Model Evaluation and Calibration\n",
    "\n",
    "Initial Isolation Forest results show low recall for labeled anomalies. Although overall accuracy is high due to class imbalance, anomaly detection performance remains weak.\n",
    "\n",
    "This indicates that our feature space does not sufficiently capture temporal behavior patterns.\n",
    "\n",
    "Next, we refine the anomaly engine by improving baseline modeling and adjusting detection granularity. In particular, we incorporate longer seasonal windows (weekly patterns) to better capture normal electricity consumption cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5aaa072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve Baseline (Weekly)\n",
    "# Change rolling window from 24 → 168 (7 days).\n",
    "\n",
    "window_size = 168  # 7 days\n",
    "\n",
    "lead_df[\"rolling_mean_week\"] = (\n",
    "    lead_df.groupby(\"building_id\")[\"meter_reading\"]\n",
    "    .transform(lambda x: x.rolling(window_size, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "lead_df[\"rolling_std_week\"] = (\n",
    "    lead_df.groupby(\"building_id\")[\"meter_reading\"]\n",
    "    .transform(lambda x: x.rolling(window_size, min_periods=1).std())\n",
    ")\n",
    "\n",
    "lead_df[\"residual_week\"] = lead_df[\"meter_reading\"] - lead_df[\"rolling_mean_week\"]\n",
    "\n",
    "lead_df[\"z_score_week\"] = (\n",
    "    lead_df[\"residual_week\"] / (lead_df[\"rolling_std_week\"] + 1e-6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242c53a",
   "metadata": {},
   "source": [
    "## Weekly Baseline Enhancement\n",
    "\n",
    "Electricity consumption typically follows weekly cycles rather than only daily patterns.  \n",
    "Using a 24-hour rolling window may not capture behavioral seasonality effectively.\n",
    "\n",
    "To improve anomaly detection performance, we compute:\n",
    "\n",
    "- 168-hour (7-day) rolling mean\n",
    "- 168-hour rolling standard deviation\n",
    "- Weekly residual (actual − expected)\n",
    "- Weekly standardized deviation (z-score)\n",
    "\n",
    "This allows the model to detect sustained deviations from weekly behavioral norms rather than short-term fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e9c337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1684011   28187]\n",
      " [  30493    6803]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98   1712198\n",
      "           1       0.19      0.18      0.19     37296\n",
      "\n",
      "    accuracy                           0.97   1749494\n",
      "   macro avg       0.59      0.58      0.59   1749494\n",
      "weighted avg       0.97      0.97      0.97   1749494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature selection (weekly features only)\n",
    "features_week = [\n",
    "    \"residual_week\",\n",
    "    \"z_score_week\",\n",
    "    \"hour\",\n",
    "    \"day_of_week\",\n",
    "    \"month\"\n",
    "]\n",
    "\n",
    "X_week = lead_df[features_week].fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler_week = StandardScaler()\n",
    "X_week_scaled = scaler_week.fit_transform(X_week)\n",
    "\n",
    "# Isolation Forest\n",
    "iso_week = IsolationForest(\n",
    "    n_estimators=150,\n",
    "    contamination=0.02,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lead_df[\"iso_week_pred\"] = iso_week.fit_predict(X_week_scaled)\n",
    "\n",
    "# Convert -1 → anomaly(1), 1 → normal(0)\n",
    "lead_df[\"iso_week_pred\"] = lead_df[\"iso_week_pred\"].map({1: 0, -1: 1})\n",
    "\n",
    "# Evaluation\n",
    "print(confusion_matrix(lead_df[\"anomaly\"], lead_df[\"iso_week_pred\"]))\n",
    "print(classification_report(lead_df[\"anomaly\"], lead_df[\"iso_week_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8dd89b",
   "metadata": {},
   "source": [
    "## Improved Weekly Isolation Forest Results\n",
    "\n",
    "After incorporating a 168-hour rolling baseline to capture weekly seasonal patterns, anomaly detection performance improved significantly.\n",
    "\n",
    "For the anomaly class:\n",
    "\n",
    "- Precision: 0.19  \n",
    "- Recall: 0.18  \n",
    "- F1-score: 0.19  \n",
    "\n",
    "Compared to the daily baseline model (F1-score ≈ 0.06), the weekly model achieves approximately a threefold improvement in anomaly detection performance.\n",
    "\n",
    "This demonstrates that feature engineering and appropriate temporal modeling are more impactful than algorithm complexity in anomaly detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59bf46",
   "metadata": {},
   "source": [
    "## Per-Building Isolation Forest Modeling\n",
    "\n",
    "Electricity consumption behavior differs significantly across buildings. \n",
    "Applying a single global anomaly detection model may fail to capture building-specific behavioral patterns.\n",
    "\n",
    "To improve detection performance, we train an Isolation Forest model separately for each building.\n",
    "\n",
    "This approach:\n",
    "\n",
    "- Accounts for building-level variability\n",
    "- Reduces cross-building distribution distortion\n",
    "- Improves anomaly sensitivity\n",
    "- Mimics real-world smart meter anomaly pipelines\n",
    "\n",
    "Each building is treated as its own anomaly detection environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94b775bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We store predictions here\n",
    "lead_df[\"iso_building_pred\"] = 0\n",
    "\n",
    "features_building = [\n",
    "    \"residual_week\",\n",
    "    \"z_score_week\",\n",
    "    \"hour\",\n",
    "    \"day_of_week\",\n",
    "    \"month\"\n",
    "]\n",
    "\n",
    "for b_id in lead_df[\"building_id\"].unique():\n",
    "    \n",
    "    building_data = lead_df[lead_df[\"building_id\"] == b_id]\n",
    "    \n",
    "    X_b = building_data[features_building].fillna(0)\n",
    "    \n",
    "    scaler_b = StandardScaler()\n",
    "    X_b_scaled = scaler_b.fit_transform(X_b)\n",
    "    \n",
    "    iso_b = IsolationForest(\n",
    "        n_estimators=150,\n",
    "        contamination=0.02,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    preds = iso_b.fit_predict(X_b_scaled)\n",
    "    \n",
    "    # Convert: -1 → anomaly(1), 1 → normal(0)\n",
    "    preds = pd.Series(preds).map({1: 0, -1: 1}).values\n",
    "    \n",
    "    lead_df.loc[building_data.index, \"iso_building_pred\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fea97abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1689995   22203]\n",
      " [  24424   12872]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99   1712198\n",
      "           1       0.37      0.35      0.36     37296\n",
      "\n",
      "    accuracy                           0.97   1749494\n",
      "   macro avg       0.68      0.67      0.67   1749494\n",
      "weighted avg       0.97      0.97      0.97   1749494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(lead_df[\"anomaly\"], lead_df[\"iso_building_pred\"]))\n",
    "print(classification_report(lead_df[\"anomaly\"], lead_df[\"iso_building_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0d574",
   "metadata": {},
   "source": [
    "## Per-Building Isolation Forest Results\n",
    "\n",
    "Training Isolation Forest models separately per building significantly improved anomaly detection performance.\n",
    "\n",
    "For the anomaly class:\n",
    "\n",
    "- Precision: 0.37  \n",
    "- Recall: 0.35  \n",
    "- F1-score: 0.36  \n",
    "\n",
    "Compared to the global model, per-building modeling accounts for building-specific consumption behavior and reduces cross-distribution distortion.\n",
    "\n",
    "This demonstrates that distribution-aware modeling is critical in electricity anomaly detection systems.\n",
    "\n",
    "The per-building Isolation Forest model will serve as the calibrated anomaly engine for subsequent deployment phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19708233",
   "metadata": {},
   "source": [
    "## Business-Oriented Evaluation — Precision at Top-K\n",
    "\n",
    "While recall and F1-score provide statistical evaluation of anomaly detection performance, utilities are primarily concerned with inspection efficiency.\n",
    "\n",
    "In real-world electricity theft detection systems, only a limited number of meters can be inspected daily due to operational constraints.\n",
    "\n",
    "Therefore, we evaluate model performance using Precision@Top-K, which answers:\n",
    "\n",
    "> “Among the top K most suspicious buildings flagged by the model, how many are actual anomalies?”\n",
    "\n",
    "We simulate inspection prioritization by ranking observations according to anomaly score and computing precision at different inspection levels (e.g., top 1%, 2%, 5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747c79f",
   "metadata": {},
   "source": [
    "### Fixing Anomaly Score Computation\n",
    "\n",
    "Because we trained Isolation Forest models per building, anomaly scores must also be computed per building.  \n",
    "Attempting to assign scores from a single building to the full dataset causes a length mismatch error.\n",
    "\n",
    "Next, we compute and store anomaly scores within each building group, then rank all observations globally to evaluate Precision@Top-K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7fc8664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing anomaly_score: 0\n",
      "Score range: -0.16409149398236877 to 0.24314405589976734\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create empty score column\n",
    "lead_df[\"anomaly_score\"] = np.nan\n",
    "\n",
    "features_building = [\"residual_week\", \"z_score_week\", \"hour\", \"day_of_week\", \"month\"]\n",
    "\n",
    "for b_id in lead_df[\"building_id\"].unique():\n",
    "    building_data = lead_df[lead_df[\"building_id\"] == b_id]\n",
    "    X_b = building_data[features_building].fillna(0)\n",
    "\n",
    "    scaler_b = StandardScaler()\n",
    "    X_b_scaled = scaler_b.fit_transform(X_b)\n",
    "\n",
    "    iso_b = IsolationForest(\n",
    "        n_estimators=150,\n",
    "        contamination=0.02,\n",
    "        random_state=42\n",
    "    )\n",
    "    iso_b.fit(X_b_scaled)\n",
    "\n",
    "    # decision_function: higher = more normal, lower = more anomalous\n",
    "    scores = iso_b.decision_function(X_b_scaled)\n",
    "\n",
    "    lead_df.loc[building_data.index, \"anomaly_score\"] = scores\n",
    "\n",
    "# Sanity check\n",
    "print(\"Missing anomaly_score:\", lead_df[\"anomaly_score\"].isna().sum())\n",
    "print(\"Score range:\", lead_df[\"anomaly_score\"].min(), \"to\", lead_df[\"anomaly_score\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92b46108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1%: 0.564\n",
      "Precision@2%: 0.368\n",
      "Precision@5%: 0.188\n"
     ]
    }
   ],
   "source": [
    "# Compute Precision@Top-K\n",
    "# Rank by most anomalous (lowest score first)\n",
    "lead_df_sorted = lead_df.sort_values(\"anomaly_score\", ascending=True)\n",
    "\n",
    "def precision_at_k_percent(df, k_percent):\n",
    "    k = int(len(df) * k_percent)\n",
    "    top_k = df.head(k)\n",
    "    return top_k[\"anomaly\"].mean()\n",
    "\n",
    "for pct in [0.01, 0.02, 0.05]:\n",
    "    print(f\"Precision@{int(pct*100)}%:\", round(precision_at_k_percent(lead_df_sorted, pct), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500ea11",
   "metadata": {},
   "source": [
    "## Precision@Top-K Results (Inspection Prioritization)\n",
    "\n",
    "To align model evaluation with real utility operations, we computed Precision@Top-K by ranking observations using the per-building anomaly score (lowest score = most anomalous).\n",
    "\n",
    "Results:\n",
    "\n",
    "- Precision@1% = 0.564  \n",
    "- Precision@2% = 0.368  \n",
    "- Precision@5% = 0.188  \n",
    "\n",
    "These results show that the model performs best as an inspection prioritization tool: the highest-risk cases (top-ranked anomalies) contain a much higher proportion of true anomalies than the overall dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
